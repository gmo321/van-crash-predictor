{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/07 17:36:24 WARN Utils: Your hostname, Glorias-MacBook-Air-10.local resolves to a loopback address: 127.0.0.1; using 10.0.0.117 instead (on interface en0)\n",
      "25/04/07 17:36:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/07 17:36:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/07 17:36:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Car Crash Prediction Model\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|hotspot_risk_level|count|\n",
      "+------------------+-----+\n",
      "|          moderate| 7691|\n",
      "|               low|12788|\n",
      "|              high| 5392|\n",
      "+------------------+-----+\n",
      "\n",
      "root\n",
      " |-- lat_bin: double (nullable = true)\n",
      " |-- lon_bin: double (nullable = true)\n",
      " |-- time_period: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- total_crashes: long (nullable = true)\n",
      " |-- crash_severity: string (nullable = true)\n",
      " |-- avg_severity_weight: double (nullable = true)\n",
      " |-- weather: string (nullable = true)\n",
      " |-- avg_speed_limit: double (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- road_condition: string (nullable = true)\n",
      " |-- road_surface: string (nullable = true)\n",
      " |-- avg_total_vehicles_involved: double (nullable = true)\n",
      " |-- avg_total_casualty: double (nullable = true)\n",
      " |-- pct_intersection_crash: double (nullable = true)\n",
      " |-- pct_pedestrian_involved: double (nullable = true)\n",
      " |-- pct_distraction_involved: double (nullable = true)\n",
      " |-- pct_drug_involved: double (nullable = true)\n",
      " |-- pct_impaired_involved: double (nullable = true)\n",
      " |-- pct_speed_involved: double (nullable = true)\n",
      " |-- pct_is_weekend: double (nullable = true)\n",
      " |-- pct_is_rush_hour: double (nullable = true)\n",
      " |-- avg_temp: double (nullable = true)\n",
      " |-- avg_visibility: double (nullable = true)\n",
      " |-- avg_clouds: double (nullable = true)\n",
      " |-- max_rain: double (nullable = true)\n",
      " |-- max_snow: double (nullable = true)\n",
      " |-- last_weather: string (nullable = true)\n",
      " |-- last_weather_description: string (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      " |-- avg_flow_speed: double (nullable = true)\n",
      " |-- avg_travel_time: double (nullable = true)\n",
      " |-- avg_flow_travel_time: double (nullable = true)\n",
      " |-- avg_speed_diff: double (nullable = true)\n",
      " |-- had_closure: double (nullable = true)\n",
      " |-- combined_risk_score: double (nullable = true)\n",
      " |-- hotspot_risk_level: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_all_df = spark.read.parquet(\"data/parquet/final_real_df\")\n",
    "\n",
    "# Compute a combined risk score\n",
    "merged_all_df = merged_all_df.withColumn(\n",
    "    \"combined_risk_score\",\n",
    "    (col(\"total_crashes\") * col(\"avg_severity_weight\"))\n",
    ")\n",
    "\n",
    "quantiles = merged_all_df.approxQuantile(\"combined_risk_score\", [0.5, 0.8], 0.01)\n",
    "q50, q80 = quantiles\n",
    "\n",
    "# Define risk levels based on combined score\n",
    "merged_all_df = merged_all_df.withColumn(\n",
    "    \"hotspot_risk_level\",\n",
    "    when((col(\"combined_risk_score\") > q80), \"high\")\n",
    "    .when((col(\"combined_risk_score\") > q50) & (col(\"combined_risk_score\") <= q80), \"moderate\")\n",
    "    .otherwise(\"low\")\n",
    ")\n",
    "\n",
    "merged_all_df.groupBy(\"hotspot_risk_level\").count().show()\n",
    "merged_all_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 Baseline Model: RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8184\n",
      "Precision: 0.8126\n",
      "Recall: 0.8184\n",
      "F1 Score: 0.8121\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# Step 1: StringIndex all categorical columns (and the target)\n",
    "categorical_columns = ['municipality', 'time_period', 'weather', 'season', 'road_condition', 'last_weather', 'last_weather_description']\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid='keep') for col in categorical_columns]\n",
    "\n",
    "# Step 2: OneHotEncode all indexed categorical columns\n",
    "encoder = OneHotEncoder(inputCols=[col + \"_index\" for col in categorical_columns], \n",
    "                        outputCols=[col + \"_ohe\" for col in categorical_columns])\n",
    "\n",
    "# Step 3: StringIndex the target variable\n",
    "indexer_target = StringIndexer(inputCol=\"hotspot_risk_level\", outputCol=\"label\", handleInvalid='keep')\n",
    "\n",
    "# Step 4: Assemble all features into a single vector\n",
    "numerical_columns = ['lat_bin', 'lon_bin', 'avg_speed_limit', 'avg_total_vehicles_involved', \n",
    "                     'avg_total_casualty', 'pct_intersection_crash', 'pct_pedestrian_involved', 'pct_distraction_involved', \n",
    "                     'pct_drug_involved', 'pct_impaired_involved', 'pct_speed_involved', 'pct_is_weekend', 'pct_is_rush_hour', \n",
    "                     'avg_temp', 'avg_visibility', 'avg_clouds', 'max_rain', 'max_snow', 'avg_speed', 'avg_flow_speed', \n",
    "                     'avg_travel_time', 'avg_flow_travel_time', 'avg_speed_diff', 'had_closure']\n",
    "\n",
    "feature_cols = numerical_columns + [col + \"_ohe\" for col in categorical_columns]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid='keep')\n",
    "\n",
    "# Apply assembler first, then fit and apply scaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_df, test_df = merged_all_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 5: Create the RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"scaled_features\", numTrees=100, maxDepth=10, seed=42)\n",
    "\n",
    "# Step 6: Build the pipeline\n",
    "pipeline_rf = Pipeline(stages=indexers + [indexer_target, encoder, assembler, scaler, rf])\n",
    "\n",
    "\n",
    "# Step 7: Train the model\n",
    "rfc_model = pipeline_rf.fit(train_df)\n",
    "\n",
    "\n",
    "# Step 8: Make predictions\n",
    "predictions_rf = rfc_model.transform(test_df)\n",
    "\n",
    "\n",
    "# Step 9: Evaluate the model\n",
    "# Create evaluators for Precision, Recall, and F1 Score\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = evaluator_accuracy.evaluate(predictions_rf)\n",
    "precision = evaluator_precision.evaluate(predictions_rf)\n",
    "recall = evaluator_recall.evaluate(predictions_rf)\n",
    "f1_score = evaluator_f1.evaluate(predictions_rf)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rfc_model.stages[-1]\n",
    "rf_model.write().overwrite().save(\"model/rf_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RFC Model\n",
    "pipeline_rf.write().overwrite().save(\"model/rfc_pipeline\")\n",
    "\n",
    "rfc_model.write().overwrite().save(\"model/rfc_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the prediction and label columns\n",
    "prediction_and_labels = predictions_rf.select(\"prediction\", \"label\").rdd\n",
    "\n",
    "# Create the confusion matrix\n",
    "metrics = MulticlassMetrics(prediction_and_labels)\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Get the confusion matrix as a DenseMatrix\n",
    "confusion_matrix_arr = metrics.confusionMatrix().toArray()\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix_arr, annot=True, fmt=\"g\", cmap=\"Blues\", xticklabels=[\"Low\", \"Moderate\", \"High\"], yticklabels=[\"Low\", \"Moderate\", \"High\"])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Convert prediction + label to RDD of (prediction, label)\n",
    "predictionAndLabels = predictions_rf.select(\"prediction\", \"label\") \\\n",
    "    .rdd.map(lambda row: (float(row.prediction), float(row.label)))\n",
    "\n",
    "# Create metrics object\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Get all labels\n",
    "labels = predictions_rf.select(\"label\").distinct().orderBy(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Compute metrics\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1 = metrics.fMeasure(label)\n",
    "    \n",
    "    print(f\"Class {int(label)}:\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Create a parameter grid for Random Forest\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100, 150]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# 5-fold Cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline_rf, \n",
    "                          estimatorParamMaps=param_grid, \n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\"), \n",
    "                          numFolds=5)  \n",
    "\n",
    "# Fit the model\n",
    "cv_model = crossval.fit(train_df)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = cv_model.bestModel\n",
    "predictions = cv_model.transform(test_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Cross-validated Model Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyper-param tuned CV model\n",
    "cv_model.write().overwrite().save(\"model/cv_model\")\n",
    "\n",
    "best_model.write().overwrite().save(\"model/best_rfc_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the prediction and label columns\n",
    "prediction_and_labels = predictions.select(\"prediction\", \"label\").rdd\n",
    "\n",
    "# Create the confusion matrix\n",
    "metrics = MulticlassMetrics(prediction_and_labels)\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Extract the prediction and label columns\n",
    "prediction_and_labels = predictions.select(\"prediction\", \"label\").rdd\n",
    "\n",
    "# Create the confusion matrix\n",
    "metrics = MulticlassMetrics(prediction_and_labels)\n",
    "\n",
    "# Get the confusion matrix as a DenseMatrix\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"g\", cmap=\"Blues\", xticklabels=[\"Low\", \"Moderate\", \"High\"], yticklabels=[\"Low\", \"Moderate\", \"High\"])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix for CV')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the trained random forest model\n",
    "rf_model = rfc_model.stages[-1]\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.featureImportances\n",
    "\n",
    "# Combine feature names with their importances\n",
    "features = numerical_columns + [col + \"_index\" for col in categorical_columns]\n",
    "feature_importance_dict = dict(zip(features, feature_importances))\n",
    "\n",
    "# Sort the feature importance\n",
    "sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract feature names and their corresponding importance values\n",
    "sorted_features = [item[0] for item in sorted_feature_importance]\n",
    "sorted_importances = [item[1] for item in sorted_feature_importance]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_features, sorted_importances, color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.gca().invert_yaxis() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Assuming numerical_columns and categorical_columns are already defined\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.featureImportances\n",
    "\n",
    "# List of features with their importance\n",
    "features_with_importance = list(zip(numerical_columns + [col + \"_index\" for col in categorical_columns], feature_importances))\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "sorted_features = sorted(features_with_importance, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select top N important features (you can adjust N based on your needs)\n",
    "top_n_features = [feature for feature, importance in sorted_features[:10]]  # For example, top 10 features\n",
    "\n",
    "# Now, filter the data to include only the selected top features\n",
    "# Ensure the list of top_n_features includes both numerical and categorical columns (with indexing)\n",
    "numeric_features = numerical_columns + [col + \"_index\" for col in categorical_columns]\n",
    "\n",
    "# Assembling features using VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=top_n_features, outputCol=\"features\")\n",
    "\n",
    "# Optionally scale the features (if you want to scale the features for Random Forest)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"scaled_features\", numTrees=100, maxDepth=10, maxBins=32)\n",
    "\n",
    "# Define the pipeline with assembling, scaling, and classification stages\n",
    "pipeline_retrained = Pipeline(stages=indexers + [indexer_target, assembler, scaler, rf])\n",
    "\n",
    "# Fit the model\n",
    "model_retrained = pipeline_retrained.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions_retrained = model_retrained.transform(test_df)\n",
    "\n",
    "# Evaluate the retrained model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_retrained = evaluator.evaluate(predictions_retrained)\n",
    "print(f\"Retrained Model Accuracy (using top {len(top_n_features)} features): {accuracy_retrained:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_retrained.write().overwrite().save(\"model/feature_importance_model\")\n",
    "\n",
    "pipeline_retrained.write().overwrite().save(\"model/feature_importance_pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_retrained = predictions_retrained.withColumnRenamed(\"lat_bin\", \"latitude\") \\\n",
    "                        .withColumnRenamed(\"lon_bin\", \"longitude\")\n",
    "        \n",
    "predictions_retrained.select(\"latitude\", \"longitude\", \"municipality\", \"hotspot_risk_level\", \"probability\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define a mapping from prediction indices to 'hotspot_risk_level'\n",
    "mapping = {0.0: 'low', 1.0: 'moderate', 2.0: 'high'}\n",
    "\n",
    "# Create a new column with mapped hotspot_risk_level based on prediction\n",
    "predictions_mapped = predictions_retrained.withColumn(\n",
    "    \"predicted_hotspot_risk_level\", \n",
    "    F.when(F.col(\"prediction\") == 0.0, 'low')\n",
    "     .when(F.col(\"prediction\") == 1.0, 'moderate')\n",
    "     .when(F.col(\"prediction\") == 2.0, 'high')\n",
    "     .otherwise(\"unknown\") \n",
    ")\n",
    "\n",
    "# Show the updated dataframe\n",
    "#predictions_mapped.select(\"hotspot_risk_level\", \"prediction\", \"predicted_hotspot_risk_level\", \"probability\").filter(col(\"predicted_hotspot_risk_level\") == \"high\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Step 1: StringIndex all categorical columns (and the target)\n",
    "categorical_columns = ['municipality', 'weather', 'season', 'road_condition', 'last_weather', 'last_weather_description']\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid='keep') for col in categorical_columns]\n",
    "\n",
    "# Step 2: OneHotEncode all indexed categorical columns\n",
    "encoder = OneHotEncoder(inputCols=[col + \"_index\" for col in categorical_columns], \n",
    "                        outputCols=[col + \"_ohe\" for col in categorical_columns])\n",
    "\n",
    "# Step 3: StringIndex the target variable\n",
    "indexer_target = StringIndexer(inputCol=\"hotspot_risk_level\", outputCol=\"label\", handleInvalid='keep')\n",
    "\n",
    "# Step 4: Assemble all features into a single vector\n",
    "numerical_columns = ['lat_bin', 'lon_bin', 'avg_speed_limit', 'avg_total_vehicles_involved', \n",
    "                     'avg_total_casualty', 'pct_intersection_crash', 'pct_pedestrian_involved', 'pct_distraction_involved', \n",
    "                     'pct_drug_involved', 'pct_impaired_involved', 'pct_speed_involved', 'pct_is_weekend', 'pct_is_rush_hour', \n",
    "                     'avg_temp', 'avg_visibility', 'avg_clouds', 'max_rain', 'max_snow', 'avg_speed', 'avg_flow_speed', \n",
    "                     'avg_travel_time', 'avg_flow_travel_time', 'avg_speed_diff', 'had_closure']\n",
    "\n",
    "feature_cols = numerical_columns + [col + \"_ohe\" for col in categorical_columns]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid='keep')\n",
    "\n",
    "# Step 5: Scale the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# Step 6: Build the pipeline\n",
    "pipeline = Pipeline(stages=indexers + [indexer_target, encoder, assembler, scaler])\n",
    "\n",
    "# Step 7: Split data into training and test sets\n",
    "train_df, test_df = merged_all_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 8: Fit the pipeline\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# Step 9: Transform the data\n",
    "train_data = pipeline_model.transform(train_df)\n",
    "test_data = pipeline_model.transform(test_df)\n",
    "\n",
    "# Step 10: Prepare XGBoost data in DMatrix format\n",
    "train_features = np.array([x.toArray() for x in train_data.select(\"scaled_features\").rdd.map(lambda row: row[0]).collect()])\n",
    "train_labels = np.array(train_data.select(\"label\").rdd.map(lambda row: row[0]).collect())\n",
    "\n",
    "test_features = np.array([x.toArray() for x in test_data.select(\"scaled_features\").rdd.map(lambda row: row[0]).collect()])\n",
    "test_labels = np.array(test_data.select(\"label\").rdd.map(lambda row: row[0]).collect())\n",
    "\n",
    "# Create DMatrix\n",
    "dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
    "dtest = xgb.DMatrix(test_features, label=test_labels)\n",
    "\n",
    "# Step 11: Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax', \n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'merror'\n",
    "}\n",
    "\n",
    "# Step 12: Train the XGBoost model\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Step 13: Make predictions\n",
    "predictions = xgb_model.predict(dtest)\n",
    "\n",
    "# Step 14: Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions, average='weighted')\n",
    "recall = recall_score(test_labels, predictions, average='weighted')\n",
    "f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEATMAP VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to Pandas\n",
    "heatmap_df = predictions_mapped.select(\"latitude\", \"longitude\", \"predicted_hotspot_risk_level\").toPandas()\n",
    "\n",
    "# Map predictions back to labels\n",
    "label_map = {0.0: \"low\", 1.0: \"moderate\", 2.0: \"high\"}\n",
    "heatmap_df[\"hotspot_risk_level\"] = heatmap_df[\"predicted_hotspot_risk_level\"].map(label_map)\n",
    "\n",
    "# Assign weight (optional: to enhance heatmap intensity)\n",
    "risk_weight = {\"low\": 1, \"moderate\": 3, \"high\": 5}\n",
    "heatmap_df[\"weight\"] = heatmap_df[\"predicted_hotspot_risk_level\"].map(risk_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Initialize map centered around average lat/lon\n",
    "center_lat = heatmap_df[\"latitude\"].mean()\n",
    "center_lon = heatmap_df[\"longitude\"].mean()\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=10)\n",
    "\n",
    "# Prepare data: [lat, lon, weight]\n",
    "heat_data = heatmap_df[[\"latitude\", \"longitude\", \"weight\"]].values.tolist()\n",
    "\n",
    "# Add heatmap layer\n",
    "HeatMap(heat_data, radius=15, blur=10, max_zoom=13).add_to(m)\n",
    "\n",
    "# Save and display\n",
    "m.save(\"hotspot_heatmap.html\")\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
